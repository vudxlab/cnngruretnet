"""
Script phÃ¢n tÃ­ch káº¿t quáº£ tá»« folder structure hiá»‡n cÃ³:
results/
â”œâ”€â”€ 5/           (output_steps=5)
â”‚   â”œâ”€â”€ conv1d_gru/
â”‚   â”œâ”€â”€ gru/
â”‚   â””â”€â”€ conv1d/
â”œâ”€â”€ 10/          (output_steps=10)
â”‚   â”œâ”€â”€ conv1d_gru/
â”‚   â”œâ”€â”€ gru/
â”‚   â””â”€â”€ conv1d/
â””â”€â”€ ...
"""

import os
import sys
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path


def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="PhÃ¢n tÃ­ch káº¿t quáº£ tá»« folder structure hiá»‡n cÃ³"
    )
    parser.add_argument(
        '--results_dir',
        type=str,
        default='results',
        help='ThÆ° má»¥c chá»©a káº¿t quáº£ (default: results/)'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='analysis',
        help='ThÆ° má»¥c lÆ°u káº¿t quáº£ phÃ¢n tÃ­ch (default: analysis/)'
    )
    parser.add_argument(
        '--plot_predictions',
        action='store_true',
        help='Táº¡o prediction comparison plots (máº¥t thÃªm thá»i gian)'
    )
    parser.add_argument(
        '--num_samples',
        type=int,
        default=5,
        help='Sá»‘ samples cho prediction plots (default: 5)'
    )
    return parser.parse_args()


def collect_metrics_from_structure(results_dir):
    """
    Thu tháº­p metrics tá»« folder structure: results/{output_step}/{model}/

    Args:
        results_dir: ThÆ° má»¥c chá»©a káº¿t quáº£

    Returns:
        pd.DataFrame: DataFrame chá»©a táº¥t cáº£ metrics
    """
    print(f"\nğŸ“Š Äang thu tháº­p metrics tá»«: {results_dir}/")
    print(f"   Cáº¥u trÃºc: {results_dir}/{{output_step}}/{{model}}/metrics.csv")

    all_metrics = []

    # Duyá»‡t qua cÃ¡c output_step folders (5, 10, 15, ...)
    for output_step_folder in os.listdir(results_dir):
        output_step_path = os.path.join(results_dir, output_step_folder)

        # Kiá»ƒm tra lÃ  folder vÃ  lÃ  sá»‘
        if not os.path.isdir(output_step_path):
            continue

        try:
            output_step = int(output_step_folder)
        except ValueError:
            print(f"  âš ï¸  Bá» qua folder: {output_step_folder} (khÃ´ng pháº£i sá»‘)")
            continue

        print(f"\n  ğŸ“‚ Output steps = {output_step}")

        # Duyá»‡t qua cÃ¡c model folders (conv1d_gru, gru, conv1d)
        for model_folder in os.listdir(output_step_path):
            model_path = os.path.join(output_step_path, model_folder)

            if not os.path.isdir(model_path):
                continue

            model = model_folder

            # Äá»c metrics.csv
            metrics_file = os.path.join(model_path, 'metrics.csv')

            if not os.path.exists(metrics_file):
                print(f"    âš ï¸  KhÃ´ng tÃ¬m tháº¥y metrics.csv trong: {model_folder}")
                continue

            try:
                df = pd.read_csv(metrics_file, encoding='utf-8')

                # ThÃªm thÃ´ng tin model vÃ  output_step
                for _, row in df.iterrows():
                    all_metrics.append({
                        'model': model,
                        'output_step': output_step,
                        'dataset': row['Dataset'],
                        'rmse': row['RMSE'],
                        'mae': row['MAE'],
                        'r2': row['R2']
                    })

                print(f"    âœ“ {model}: {len(df)} datasets")

            except Exception as e:
                print(f"    âŒ Lá»—i khi Ä‘á»c {model}: {e}")

    # Táº¡o DataFrame
    metrics_df = pd.DataFrame(all_metrics)

    if metrics_df.empty:
        print("\nâŒ KhÃ´ng thu tháº­p Ä‘Æ°á»£c metrics nÃ o!")
        return metrics_df

    print(f"\nâœ“ ÄÃ£ thu tháº­p {len(metrics_df)} entries")
    print(f"  Models: {sorted(metrics_df['model'].unique())}")
    print(f"  Output steps: {sorted(metrics_df['output_step'].unique())}")
    print(f"  Datasets: {sorted(metrics_df['dataset'].unique())}")

    return metrics_df


def create_comparison_table(metrics_df, output_dir):
    """Táº¡o báº£ng so sÃ¡nh metrics"""
    print(f"\nğŸ“‹ Äang táº¡o báº£ng so sÃ¡nh...")

    # Filter chá»‰ láº¥y Test set
    test_df = metrics_df[metrics_df['dataset'] == 'Test'].copy()

    # Sort by output_step vÃ  model
    test_df = test_df.sort_values(['output_step', 'model'])

    # Save to CSV
    output_file = os.path.join(output_dir, 'comparison_table.csv')
    test_df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"âœ“ ÄÃ£ lÆ°u báº£ng so sÃ¡nh: {output_file}")

    # Print summary
    print("\nğŸ“Š Báº¢NG SO SÃNH (Test Set):")
    print("=" * 100)
    print(f"{'Out Steps':<12} {'Model':<15} {'RMSE':<15} {'MAE':<15} {'RÂ²':<10}")
    print("-" * 100)

    for _, row in test_df.iterrows():
        print(f"{row['output_step']:<12} {row['model']:<15} "
              f"{row['rmse']:<15.6f} {row['mae']:<15.6f} {row['r2']:<10.4f}")

    print("=" * 100)

    return test_df


def plot_metrics_vs_output_steps(metrics_df, output_dir):
    """Váº½ biá»ƒu Ä‘á»“ metrics theo output_steps"""
    print(f"\nğŸ“ˆ Äang táº¡o visualization...")

    # Filter Test set
    test_df = metrics_df[metrics_df['dataset'] == 'Test'].copy()

    # Set style
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (18, 12)

    # Táº¡o 3 subplots
    fig, axes = plt.subplots(3, 1, figsize=(14, 12))

    metrics_to_plot = [
        ('r2', 'RÂ² Score', 'higher is better'),
        ('rmse', 'RMSE', 'lower is better'),
        ('mae', 'MAE', 'lower is better')
    ]

    colors = {'conv1d_gru': '#2ecc71', 'gru': '#3498db', 'conv1d': '#e74c3c'}
    markers = {'conv1d_gru': 'o', 'gru': 's', 'conv1d': '^'}
    model_names = {'conv1d_gru': 'Conv1D-GRU-ResNet', 'gru': 'GRU', 'conv1d': 'Conv1D'}

    for idx, (metric, title, note) in enumerate(metrics_to_plot):
        ax = axes[idx]

        # Plot tá»«ng model
        for model in sorted(test_df['model'].unique()):
            model_data = test_df[test_df['model'] == model].sort_values('output_step')

            model_display_name = model_names.get(model, model.upper().replace('_', '-'))
            ax.plot(
                model_data['output_step'],
                model_data[metric],
                marker=markers.get(model, 'o'),
                linewidth=2.5,
                markersize=10,
                label=model_display_name,
                color=colors.get(model, '#95a5a6')
            )

        ax.set_xlabel('Output Steps', fontsize=13, fontweight='bold')
        ax.set_ylabel(title, fontsize=13, fontweight='bold')
        ax.set_title(f'{title} vs Output Steps ({note})', fontsize=15, fontweight='bold', pad=15)
        ax.legend(fontsize=12, loc='best', framealpha=0.9)
        ax.grid(True, alpha=0.3, linestyle='--')

        # Set x-axis ticks
        output_steps = sorted(test_df['output_step'].unique())
        ax.set_xticks(output_steps)

        # Format y-axis
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.4f}'))

    plt.tight_layout()

    # Save plot
    output_file = os.path.join(output_dir, 'metrics_vs_output_steps.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ“ ÄÃ£ lÆ°u biá»ƒu Ä‘á»“: {output_file}")


def plot_heatmap(metrics_df, output_dir):
    """Váº½ heatmap cho tá»«ng metric"""
    print(f"\nğŸ”¥ Äang táº¡o heatmaps...")

    # Filter Test set
    test_df = metrics_df[metrics_df['dataset'] == 'Test'].copy()

    fig, axes = plt.subplots(1, 3, figsize=(22, 6))

    metrics_to_plot = ['r2', 'rmse', 'mae']
    titles = ['RÂ² Score\n(Higher is Better)', 'RMSE\n(Lower is Better)', 'MAE\n(Lower is Better)']
    cmaps = ['RdYlGn', 'RdYlGn_r', 'RdYlGn_r']

    for idx, (metric, title, cmap) in enumerate(zip(metrics_to_plot, titles, cmaps)):
        # Pivot table
        pivot = test_df.pivot(index='model', columns='output_step', values=metric)

        # Rename models for display
        pivot.index = [m.upper().replace('_', '-') for m in pivot.index]

        # Plot heatmap
        sns.heatmap(
            pivot,
            annot=True,
            fmt='.4f',
            cmap=cmap,
            ax=axes[idx],
            cbar_kws={'label': metric.upper()},
            linewidths=1,
            linecolor='white',
            annot_kws={'fontsize': 10}
        )

        axes[idx].set_title(title, fontsize=14, fontweight='bold', pad=15)
        axes[idx].set_xlabel('Output Steps', fontsize=12, fontweight='bold')
        axes[idx].set_ylabel('Model', fontsize=12, fontweight='bold')

        # Rotate labels
        axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)
        axes[idx].set_yticklabels(axes[idx].get_yticklabels(), rotation=0)

    plt.tight_layout()

    # Save
    output_file = os.path.join(output_dir, 'heatmaps.png')
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()

    print(f"âœ“ ÄÃ£ lÆ°u heatmaps: {output_file}")


def find_best_configurations(metrics_df, output_dir):
    """TÃ¬m cÃ¡c cáº¥u hÃ¬nh tá»‘t nháº¥t"""
    print(f"\nğŸ† Äang tÃ¬m best configurations...")

    # Filter Test set
    test_df = metrics_df[metrics_df['dataset'] == 'Test'].copy()

    results = []

    # Best RÂ² (highest)
    best_r2 = test_df.loc[test_df['r2'].idxmax()]
    results.append({
        'Metric': 'Best RÂ²',
        'Model': best_r2['model'],
        'Output Steps': int(best_r2['output_step']),
        'Value': f"{best_r2['r2']:.6f}",
        'RMSE': f"{best_r2['rmse']:.6f}",
        'MAE': f"{best_r2['mae']:.6f}"
    })

    # Best RMSE (lowest)
    best_rmse = test_df.loc[test_df['rmse'].idxmin()]
    results.append({
        'Metric': 'Best RMSE',
        'Model': best_rmse['model'],
        'Output Steps': int(best_rmse['output_step']),
        'Value': f"{best_rmse['rmse']:.6f}",
        'RMSE': f"{best_rmse['rmse']:.6f}",
        'MAE': f"{best_rmse['mae']:.6f}"
    })

    # Best MAE (lowest)
    best_mae = test_df.loc[test_df['mae'].idxmin()]
    results.append({
        'Metric': 'Best MAE',
        'Model': best_mae['model'],
        'Output Steps': int(best_mae['output_step']),
        'Value': f"{best_mae['mae']:.6f}",
        'RMSE': f"{best_mae['rmse']:.6f}",
        'MAE': f"{best_mae['mae']:.6f}"
    })

    # Save to CSV
    best_df = pd.DataFrame(results)
    output_file = os.path.join(output_dir, 'best_configurations.csv')
    best_df.to_csv(output_file, index=False, encoding='utf-8')

    print(f"âœ“ ÄÃ£ lÆ°u best configurations: {output_file}")

    # Print
    print("\nğŸ† BEST CONFIGURATIONS:")
    print("=" * 100)
    for result in results:
        print(f"\n{result['Metric']}:")
        print(f"  Model: {result['Model'].upper()}")
        print(f"  Output Steps: {result['Output Steps']}")
        print(f"  Value: {result['Value']}")
        print(f"  RMSE: {result['RMSE']}, MAE: {result['MAE']}")
    print("=" * 100)

    return best_df


def generate_summary_report(metrics_df, output_dir):
    """Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p"""
    print(f"\nğŸ“ Äang táº¡o bÃ¡o cÃ¡o tá»•ng há»£p...")

    # Filter Test set
    test_df = metrics_df[metrics_df['dataset'] == 'Test'].copy()

    report = []
    report.append("=" * 100)
    report.append("BÃO CÃO SO SÃNH MODELS Vá»šI CÃC OUTPUT_STEPS")
    report.append("=" * 100)
    report.append("")

    # General stats
    report.append(f"Tá»•ng sá»‘ models: {len(test_df['model'].unique())}")
    report.append(f"Models: {', '.join([m.upper() for m in sorted(test_df['model'].unique())])}")
    report.append(f"Output steps: {sorted(test_df['output_step'].unique())}")
    report.append(f"Tá»•ng sá»‘ combinations: {len(test_df)}")
    report.append("")

    # Performance summary by model
    report.append("=" * 100)
    report.append("PERFORMANCE SUMMARY BY MODEL (Test Set)")
    report.append("=" * 100)
    report.append("")

    for model in sorted(test_df['model'].unique()):
        model_data = test_df[test_df['model'] == model].sort_values('output_step')

        report.append(f"\n{model.upper().replace('_', '-')}:")
        report.append(f"{'':4}RÂ² Range: {model_data['r2'].min():.6f} â†’ {model_data['r2'].max():.6f}")
        report.append(f"{'':4}RMSE Range: {model_data['rmse'].min():.6f} â†’ {model_data['rmse'].max():.6f}")
        report.append(f"{'':4}MAE Range: {model_data['mae'].min():.6f} â†’ {model_data['mae'].max():.6f}")

        # Best output_step for this model
        best_idx = model_data['r2'].idxmax()
        best = model_data.loc[best_idx]
        worst_idx = model_data['r2'].idxmin()
        worst = model_data.loc[worst_idx]

        report.append(f"{'':4}Best output_step: {int(best['output_step'])} "
                     f"(RÂ²={best['r2']:.6f}, RMSE={best['rmse']:.6f})")
        report.append(f"{'':4}Worst output_step: {int(worst['output_step'])} "
                     f"(RÂ²={worst['r2']:.6f}, RMSE={worst['rmse']:.6f})")

        # Performance degradation
        perf_degradation = ((best['r2'] - worst['r2']) / best['r2']) * 100
        report.append(f"{'':4}Performance degradation: {perf_degradation:.2f}%")

    report.append("")
    report.append("=" * 100)

    # Impact of output_steps
    report.append("IMPACT OF OUTPUT_STEPS ON AVERAGE PERFORMANCE")
    report.append("=" * 100)
    report.append("")

    for out_step in sorted(test_df['output_step'].unique()):
        step_data = test_df[test_df['output_step'] == out_step]

        report.append(f"\nOutput Steps = {out_step}:")
        report.append(f"{'':4}Average RÂ²: {step_data['r2'].mean():.6f} Â± {step_data['r2'].std():.6f}")
        report.append(f"{'':4}Average RMSE: {step_data['rmse'].mean():.6f} Â± {step_data['rmse'].std():.6f}")
        report.append(f"{'':4}Average MAE: {step_data['mae'].mean():.6f} Â± {step_data['mae'].std():.6f}")

        # Best model for this output_step
        best_idx = step_data['r2'].idxmax()
        best = step_data.loc[best_idx]
        report.append(f"{'':4}Best model: {best['model'].upper()} (RÂ²={best['r2']:.6f})")

    report.append("")
    report.append("=" * 100)

    # Save report
    output_file = os.path.join(output_dir, 'summary_report.txt')
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report))

    print(f"âœ“ ÄÃ£ lÆ°u bÃ¡o cÃ¡o: {output_file}")

    # Print to console
    print("\n" + '\n'.join(report))


def plot_training_curves_comparison(results_dir, output_dir):
    """
    Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh training curves (loss) cá»§a 3 models cho má»—i output_step

    Args:
        results_dir: ThÆ° má»¥c chá»©a káº¿t quáº£
        output_dir: ThÆ° má»¥c output
    """
    import pickle

    print(f"\nğŸ“ˆ Äang táº¡o training curves comparison...")

    # Detect models vÃ  output_steps
    models = []
    output_steps = []

    for folder in os.listdir(results_dir):
        folder_path = os.path.join(results_dir, folder)
        if not os.path.isdir(folder_path):
            continue

        try:
            out_step = int(folder)
            output_steps.append(out_step)

            for model_folder in os.listdir(folder_path):
                if os.path.isdir(os.path.join(folder_path, model_folder)):
                    if model_folder not in models:
                        models.append(model_folder)
        except ValueError:
            continue

    models = sorted(models)
    output_steps = sorted(output_steps)

    if not models or not output_steps:
        print("\nâš ï¸  KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")
        return

    # Colors cho tá»«ng model
    colors = {
        'conv1d_gru': '#2ecc71',  # Xanh lÃ¡ - Conv1D-GRU-ResNet
        'gru': '#3498db',          # Xanh dÆ°Æ¡ng - GRU
        'conv1d': '#e74c3c'        # Äá» - Conv1D
    }

    # Model name mapping
    model_names = {
        'conv1d_gru': 'Conv1D-GRU-ResNet',
        'gru': 'GRU',
        'conv1d': 'Conv1D'
    }

    # Táº¡o folder output
    curves_dir = os.path.join(output_dir, 'training_curves')
    os.makedirs(curves_dir, exist_ok=True)

    # Váº½ cho tá»«ng output_step
    for out_step in output_steps:
        print(f"\n  ğŸ“Š Output step = {out_step}")

        # Load history cho táº¥t cáº£ models
        histories = {}

        for model in models:
            history_file = os.path.join(results_dir, str(out_step), model, 'history_saved.pkl')

            if not os.path.exists(history_file):
                print(f"    âš ï¸  KhÃ´ng tÃ¬m tháº¥y history: {model}")
                continue

            try:
                with open(history_file, 'rb') as f:
                    history = pickle.load(f)
                histories[model] = history
                print(f"    âœ“ Loaded {model}: {len(history.get('loss', []))} epochs")
            except Exception as e:
                print(f"    âŒ Lá»—i load {model}: {e}")

        if not histories:
            print(f"    âš ï¸  KhÃ´ng cÃ³ history nÃ o cho output_step={out_step}")
            continue

        # Váº½ biá»ƒu Ä‘á»“ (2 subplots: Train Loss & Val Loss)
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # Subplot 1: Training Loss
        ax1 = axes[0]
        for model, history in histories.items():
            if 'loss' in history:
                epochs = range(1, len(history['loss']) + 1)
                model_display_name = model_names.get(model, model.upper().replace('_', '-'))
                ax1.plot(epochs, history['loss'],
                        linewidth=2, label=model_display_name,
                        color=colors.get(model, '#95a5a6'), alpha=0.8)

        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Loss', fontsize=12, fontweight='bold')
        ax1.set_title(f'Training Loss - Output Steps = {out_step}',
                     fontsize=14, fontweight='bold', pad=15)
        ax1.legend(fontsize=11, loc='best')
        ax1.grid(True, alpha=0.3, linestyle='--')
        ax1.set_yscale('log')  # Log scale cho loss

        # Subplot 2: Validation Loss
        ax2 = axes[1]
        for model, history in histories.items():
            if 'val_loss' in history:
                epochs = range(1, len(history['val_loss']) + 1)
                model_display_name = model_names.get(model, model.upper().replace('_', '-'))
                ax2.plot(epochs, history['val_loss'],
                        linewidth=2, label=model_display_name,
                        color=colors.get(model, '#95a5a6'), alpha=0.8)

        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Validation Loss', fontsize=12, fontweight='bold')
        ax2.set_title(f'Validation Loss - Output Steps = {out_step}',
                     fontsize=14, fontweight='bold', pad=15)
        ax2.legend(fontsize=11, loc='best')
        ax2.grid(True, alpha=0.3, linestyle='--')
        ax2.set_yscale('log')  # Log scale cho loss

        plt.tight_layout()

        # Save
        output_file = os.path.join(curves_dir, f'training_curves_out{out_step}.png')
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"    âœ“ ÄÃ£ lÆ°u: {output_file}")

    print(f"\nâœ“ Training curves comparison Ä‘Ã£ lÆ°u táº¡i: {curves_dir}/")


def generate_prediction_comparisons(results_dir, output_dir, num_samples=5):
    """
    Táº¡o prediction comparison visualizations

    Args:
        results_dir: ThÆ° má»¥c chá»©a káº¿t quáº£
        output_dir: ThÆ° má»¥c output
        num_samples: Sá»‘ samples Ä‘á»ƒ váº½
    """
    # Import here to avoid dependency if not needed
    try:
        from plot_prediction_comparison import (
            plot_overlay_comparison,
            plot_comparison_by_output_step,
            plot_comparison_by_model,
            plot_all_combinations_grid
        )
    except ImportError:
        print("\nâš ï¸  KhÃ´ng thá»ƒ import plot_prediction_comparison")
        return

    print("\n" + "=" * 100)
    print("  Táº O PREDICTION COMPARISON VISUALIZATIONS")
    print("=" * 100)

    # Detect models vÃ  output_steps
    models = []
    output_steps = []

    for folder in os.listdir(results_dir):
        folder_path = os.path.join(results_dir, folder)
        if not os.path.isdir(folder_path):
            continue

        try:
            out_step = int(folder)
            output_steps.append(out_step)

            for model_folder in os.listdir(folder_path):
                if os.path.isdir(os.path.join(folder_path, model_folder)):
                    if model_folder not in models:
                        models.append(model_folder)
        except ValueError:
            continue

    models = sorted(models)
    output_steps = sorted(output_steps)

    print(f"\nÄÃ£ phÃ¡t hiá»‡n:")
    print(f"  Models: {models}")
    print(f"  Output steps: {output_steps}")

    if not models or not output_steps:
        print("\nâš ï¸  KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cho predictions!")
        return

    # 1. Overlay comparison (Cáº¢ 3 models trÃªn cÃ¹ng subplot - KHUYÃŠN DÃ™NG)
    print("\n1ï¸âƒ£  Overlay Comparison (3 models on same plot - RECOMMENDED):")
    for out_step in output_steps:
        plot_overlay_comparison(results_dir, out_step, models,
                               output_dir, num_samples=num_samples)

    # 2. Comparison by output_step (separate subplots)
    print("\n2ï¸âƒ£  Comparison by Output Step (separate subplots):")
    for out_step in output_steps:
        plot_comparison_by_output_step(results_dir, out_step, models,
                                      output_dir, num_samples=num_samples)

    # 3. Comparison by model
    print("\n3ï¸âƒ£  Comparison by Model:")
    for model in models:
        plot_comparison_by_model(results_dir, model, output_steps,
                                output_dir, num_samples=num_samples)

    # 4. Grid overview
    print("\n4ï¸âƒ£  Overview Grid:")
    for sample_idx in range(min(3, num_samples)):
        plot_all_combinations_grid(results_dir, models, output_steps,
                                   output_dir, sample_idx=sample_idx)

    print(f"\nâœ… Prediction comparisons Ä‘Ã£ lÆ°u táº¡i: {output_dir}/predictions_comparison/")


def main():
    """Main function"""
    args = parse_args()

    print("=" * 100)
    print("  PHÃ‚N TÃCH Káº¾T QUáº¢ - SO SÃNH MODELS Vá»šI OUTPUT_STEPS")
    print("=" * 100)

    # Kiá»ƒm tra results_dir tá»“n táº¡i
    if not os.path.exists(args.results_dir):
        print(f"\nâŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {args.results_dir}")
        sys.exit(1)

    # Táº¡o output directory
    os.makedirs(args.output_dir, exist_ok=True)
    print(f"\nğŸ“ Output directory: {args.output_dir}/")

    # Thu tháº­p metrics
    metrics_df = collect_metrics_from_structure(args.results_dir)

    if metrics_df.empty:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y metrics nÃ o!")
        sys.exit(1)

    # Táº¡o cÃ¡c phÃ¢n tÃ­ch metrics
    create_comparison_table(metrics_df, args.output_dir)
    plot_metrics_vs_output_steps(metrics_df, args.output_dir)
    plot_heatmap(metrics_df, args.output_dir)
    find_best_configurations(metrics_df, args.output_dir)
    generate_summary_report(metrics_df, args.output_dir)

    # Táº¡o training curves comparison
    plot_training_curves_comparison(args.results_dir, args.output_dir)

    # Táº¡o prediction comparisons náº¿u Ä‘Æ°á»£c yÃªu cáº§u
    if args.plot_predictions:
        generate_prediction_comparisons(args.results_dir, args.output_dir,
                                       num_samples=args.num_samples)

    print("\n" + "=" * 100)
    print("âœ… HOÃ€N THÃ€NH PHÃ‚N TÃCH!")
    print("=" * 100)
    print(f"\nğŸ“ Káº¿t quáº£ phÃ¢n tÃ­ch lÆ°u táº¡i: {args.output_dir}/")
    print("\nCÃ¡c files Ä‘Ã£ táº¡o:")
    print("  âœ“ comparison_table.csv       # Báº£ng so sÃ¡nh Ä‘áº§y Ä‘á»§")
    print("  âœ“ metrics_vs_output_steps.png # Biá»ƒu Ä‘á»“ metrics theo output_steps")
    print("  âœ“ heatmaps.png                # Heatmaps cho cÃ¡c metrics")
    print("  âœ“ best_configurations.csv     # CÃ¡c cáº¥u hÃ¬nh tá»‘t nháº¥t")
    print("  âœ“ summary_report.txt          # BÃ¡o cÃ¡o tá»•ng há»£p")
    print("\n  ğŸ“ˆ Training Curves:")
    print("  âœ“ training_curves/training_curves_out*.png  # So sÃ¡nh loss curves cá»§a 3 models")

    if args.plot_predictions:
        print("\n  ğŸ“Š Prediction Comparisons:")
        print("  ğŸŒŸ predictions_comparison/overlay_out*.png        # Overlay 3 models (KHUYÃŠN XEM)")
        print("  âœ“ predictions_comparison/comparison_out*.png     # So sÃ¡nh models (3 subplots)")
        print("  âœ“ predictions_comparison/comparison_{model}.png  # So sÃ¡nh output_steps theo model")
        print("  âœ“ predictions_comparison/grid_sample*.png        # Grid tá»•ng quan")

    print("=" * 100)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user!")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
