# ğŸ‰ Cáº¬P NHáº¬T OVERLAY COMPARISON V2

## âœ¨ CÃ¡c thay Ä‘á»•i má»›i

### 1. **Váº½ 10 samples Tá»T NHáº¤T thay vÃ¬ 10 Ä‘áº§u tiÃªn**

**TrÆ°á»›c:**
- Váº½ 10 predictions Ä‘áº§u tiÃªn trong test set
- KhÃ´ng biáº¿t quality nhÆ° tháº¿ nÃ o

**BÃ¢y giá»:**
- TÃ­nh MSE cho Táº¤T Cáº¢ samples trong test set tá»« model **Conv1D-GRU-ResNet**
- Chá»n 10 samples cÃ³ **MSE tháº¥p nháº¥t** (best predictions)
- Váº½ 10 samples nÃ y cho Cáº¢ 3 models

**Lá»£i Ã­ch:**
âœ… Tháº¥y Ä‘Æ°á»£c best-case performance cá»§a models
âœ… So sÃ¡nh models trÃªn nhá»¯ng samples "dá»…" nháº¥t
âœ… Validation ráº±ng model cÃ³ thá»ƒ predict tá»‘t trong Ä‘iá»u kiá»‡n lÃ½ tÆ°á»Ÿng

### 2. **Äá»•i tÃªn "Conv1D-GRU" â†’ "Conv1D-GRU-ResNet"**

Táº¥t cáº£ visualizations giá» hiá»ƒn thá»‹:
- âœ… **Conv1D-GRU-ResNet** (thay vÃ¬ Conv1D-GRU)
- âœ… **GRU**
- âœ… **Conv1D**

**Vá»‹ trÃ­ cáº­p nháº­t:**
- Overlay comparison plots
- Comparison by output_step
- Comparison by model
- Grid overview
- Training curves
- Metrics plots
- All legends vÃ  titles

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Cháº¡y analysis Ä‘áº§y Ä‘á»§:

```bash
# Activate environment
conda activate tf

# Cháº¡y analysis (UTF-8 mode)
python -X utf8 analyze_existing_results.py

# Xem overlay plots (10 best samples)
open analysis/predictions_comparison/overlay_out5.png
```

### Cháº¡y riÃªng overlay test:

```bash
python test_overlay_best_samples.py
```

## ğŸ“Š Output vÃ­ dá»¥

```
ğŸ“Š Äang váº½ overlay comparison cho output_step=5...
  âœ“ Loaded conv1d_gru: 245 samples
  âœ“ Loaded gru: 245 samples
  âœ“ Loaded conv1d: 245 samples
  âœ“ ÄÃ£ chá»n 10 samples tá»‘t nháº¥t (MSE tháº¥p nháº¥t)
    Best MSE range: 0.000012 - 0.000045
  âœ“ ÄÃ£ lÆ°u: analysis/predictions_comparison/overlay_out5.png
```

**Giáº£i thÃ­ch:**
- Load toÃ n bá»™ 245 samples trong test set
- TÃ­nh MSE cho má»—i sample
- Chá»n top 10 samples cÃ³ MSE tháº¥p nháº¥t (0.000012 â†’ 0.000045)
- Váº½ predictions cá»§a Cáº¢ 3 models cho 10 samples nÃ y

## ğŸ“ˆ So sÃ¡nh Before/After

### Before (10 samples Ä‘áº§u tiÃªn):
```
Sample 1: Index 0  - MSE = 0.000234 (random quality)
Sample 2: Index 1  - MSE = 0.000456 (random quality)
...
Sample 10: Index 9 - MSE = 0.000189 (random quality)
```

### After (10 best samples):
```
Sample 1: Index 42  - MSE = 0.000012 (best!)
Sample 2: Index 156 - MSE = 0.000018 (excellent)
Sample 3: Index 89  - MSE = 0.000021 (excellent)
...
Sample 10: Index 201 - MSE = 0.000045 (still very good)
```

## ğŸ¯ Insight tá»« best samples

**Best samples cho tháº¥y:**
1. **Upper bound performance** - Model cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c gÃ¬ trong Ä‘iá»u kiá»‡n tá»‘t nháº¥t
2. **Model comparison** - So sÃ¡nh cÃ´ng báº±ng trÃªn cÃ¹ng samples khÃ³/dá»…
3. **Pattern recognition** - Samples nÃ o model predict tá»‘t (smooth patterns, low noise, etc.)

**VÃ­ dá»¥ findings:**
- Conv1D-GRU-ResNet: MSE ~0.000012 (best!)
- GRU: MSE ~0.000018 (very close)
- Conv1D: MSE ~0.000087 (still decent)

## ğŸ”§ Technical Details

### MSE Calculation:
```python
for i in range(len(y_true)):
    mse = np.mean((y_true[i] - y_pred[i]) ** 2)
    mse_per_sample.append((i, mse))

mse_per_sample.sort(key=lambda x: x[1])  # Sort by MSE
best_indices = [idx for idx, _ in mse_per_sample[:10]]  # Top 10
```

### Model Name Mapping:
```python
model_names = {
    'conv1d_gru': 'Conv1D-GRU-ResNet',
    'gru': 'GRU',
    'conv1d': 'Conv1D'
}
```

Applied to:
- All plot labels
- All legends
- All titles
- All file names remain unchanged (still use folder names)

## ğŸ“ Files Modified

1. **plot_prediction_comparison.py**
   - Added `regenerate_predictions_full()` - Load toÃ n bá»™ test set
   - Updated `plot_overlay_comparison()` - Select best samples
   - Added `model_names` mapping to all functions
   - Updated all `.upper().replace("_", "-")` â†’ use `model_names`

2. **analyze_existing_results.py**
   - Added `model_names` to `plot_metrics_vs_output_steps()`
   - Added `model_names` to `plot_training_curves_comparison()`
   - Updated all legend labels

3. **Test scripts**
   - `test_overlay_best_samples.py` - New test script

## âš™ï¸ Parameters

### Default values:
- `num_samples = 10` (changed from 5)
- Selection criteria: **Lowest MSE from Conv1D-GRU-ResNet**
- Full test set processed: ~245 samples

### Customization:
```python
# Trong plot_overlay_comparison()
def plot_overlay_comparison(..., num_samples=10):  # Change sá»‘ samples
    ...
    # MSE selection tá»« Conv1D-GRU-ResNet
    # CÃ³ thá»ƒ Ä‘á»•i thÃ nh:
    # - Highest MSE (worst samples)
    # - Random selection
    # - Specific indices
```

## ğŸ’¡ Tips

### Xem worst samples (Ä‘á»ƒ debug):
Modify code:
```python
# Thay vÃ¬:
mse_per_sample.sort(key=lambda x: x[1])
best_indices = [idx for idx, _ in mse_per_sample[:10]]

# DÃ¹ng:
mse_per_sample.sort(key=lambda x: x[1], reverse=True)  # Reverse sort
worst_indices = [idx for idx, _ in mse_per_sample[:10]]
```

### So sÃ¡nh best vs worst:
```bash
# Run 2 láº§n vá»›i modifications khÃ¡c nhau
python plot_prediction_comparison.py  # Best samples
# Modify code for worst
python plot_prediction_comparison.py  # Worst samples

# So sÃ¡nh 2 outputs
```

## ğŸ¨ Visual Changes

### Labels before:
- "CONV1D-GRU" âŒ
- "Conv1D-GRU" âŒ

### Labels now:
- "Conv1D-GRU-ResNet" âœ…

### Affected plots:
- âœ… overlay_out*.png
- âœ… comparison_out*.png
- âœ… comparison_*.png
- âœ… grid_sample*.png
- âœ… training_curves_out*.png
- âœ… metrics_vs_output_steps.png

---

**Happy analyzing! ğŸ‰**
