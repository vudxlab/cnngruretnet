# Time Series Forecasting - Vibration Data Prediction

Project dá»± Ä‘oÃ¡n dá»¯ liá»‡u rung Ä‘á»™ng tá»« cáº£m biáº¿n cÃ´ng nghiá»‡p sá»­ dá»¥ng Deep Learning.

## Kiáº¿n trÃºc Models

### Main Models
1. **CNN+ResNet+GRU** (`cnn_resnet_gru`): Hybrid model vá»›i CNN, Residual Network vÃ  GRU - **Best Performance (RÂ² ~ 0.976)**
2. **CNN** (`cnn`): CNN thuáº§n cho time series
3. **GRU** (`gru`): RNN vá»›i Gated Recurrent Unit (3 layers)

### Ablation Study Models
4. **CNN+GRU** (`cnn_gru`): CNN + GRU **KHÃ”NG CÃ“** Residual Connection (Ä‘á»ƒ test tÃ¡c Ä‘á»™ng cá»§a ResNet)
5. **CNN+ResNet** (`cnn_resnet`): CNN + ResNet **KHÃ”NG CÃ“** GRU layers (Ä‘á»ƒ test tÃ¡c Ä‘á»™ng cá»§a recurrent layers)
6. **CNN+ResNet+GRU+BN** (`cnn_resnet_gru_bn`): Full model **Vá»šI** BatchNorm/Dropout (Ä‘á»ƒ test tÃ¡c Ä‘á»™ng cá»§a regularization)
7. **CNN+ResNet+GRU (1L/2L/4L)** (`cnn_resnet_gru_var`): Variable depth models vá»›i 1, 2, hoáº·c 4 GRU layers (Ä‘á»ƒ test tÃ¡c Ä‘á»™ng cá»§a model depth)

### Baseline Models
8. **Linear Regression** (`linear`): Baseline Ä‘Æ¡n giáº£n
9. **XGBoost** (`xgboost`): Tree-based model
10. **LightGBM** (`lightgbm`): Gradient boosting model

## CÃ i Ä‘áº·t

```bash
pip install -r requirements.txt
```

## Cáº¥u trÃºc thÆ° má»¥c

```
4_Code/
â”œâ”€â”€ Data/                    # Dá»¯ liá»‡u .mat
â”‚   â””â”€â”€ TH2_SETUP1.mat      # File dá»¯ liá»‡u chÃ­nh
â”œâ”€â”€ config.py               # Cáº¥u hÃ¬nh (hyperparameters, paths)
â”œâ”€â”€ data_loader.py          # Load dá»¯ liá»‡u tá»« .mat file
â”œâ”€â”€ data_preprocessing.py   # Preprocessing & augmentation
â”œâ”€â”€ model.py                # Äá»‹nh nghÄ©a models
â”œâ”€â”€ baseline_models.py      # Baseline models (LR, XGB, LGBM)
â”œâ”€â”€ trainer.py              # Training logic
â”œâ”€â”€ evaluator.py            # Evaluation metrics
â”œâ”€â”€ visualization.py        # Váº½ biá»ƒu Ä‘á»“
â”œâ”€â”€ utils.py                # Utilities
â”œâ”€â”€ main.py                 # Entry point
â””â”€â”€ requirements.txt        # Dependencies
```

## Sá»­ dá»¥ng

### 1. Train má»™t hoáº·c nhiá»u models (CÃ¡ch má»›i - KhuyÃªn dÃ¹ng)

```bash
# Train má»™t model
python main.py --models cnn_resnet_gru

# Train nhiá»u models cÃ¹ng lÃºc
python main.py --models cnn_resnet_gru gru cnn

# Train táº¥t cáº£ Deep Learning models
python main.py --models cnn_resnet_gru gru cnn --epochs 500

# Train táº¥t cáº£ Baseline models
python main.py --models linear xgboost lightgbm

# Train Táº¤T Cáº¢ main models
python main.py --models cnn_resnet_gru gru cnn linear xgboost lightgbm
```

**LÆ°u Ã½:** Má»—i model sáº½ tá»± Ä‘á»™ng lÆ°u vÃ o thÆ° má»¥c riÃªng: `results/{model_name}/`

**CÃ¡c model types cÃ³ sáºµn:**

**Main Models:**
- `cnn_resnet_gru` - Best model (CNN+ResNet+GRU: Hybrid CNN-RNN vá»›i ResNet)
- `gru` - Pure RNN vá»›i 3 GRU layers
- `cnn` - Pure CNN

**Ablation Study Models:**
- `cnn_gru` - CNN+GRU (khÃ´ng cÃ³ residual connection)
- `cnn_resnet` - CNN+ResNet (khÃ´ng cÃ³ GRU layers)
- `cnn_resnet_gru_bn` - CNN+ResNet+GRU vá»›i BatchNorm/Dropout
- `cnn_resnet_gru_var` - CNN+ResNet+GRU vá»›i sá»‘ GRU layers tÃ¹y chá»‰nh (dÃ¹ng `--num_gru_layers`)

**Baseline Models:**
- `linear` - Linear Regression baseline
- `xgboost` - XGBoost baseline
- `lightgbm` - LightGBM baseline

### 2. Train Táº¤T Cáº¢ models báº±ng script tiá»‡n lá»£i

```bash
python train_all_models.py
```

Script nÃ y sáº½ train táº¥t cáº£ 6 models tuáº§n tá»± vÃ  hiá»ƒn thá»‹ summary chi tiáº¿t.

### 3. TÃ¹y chá»‰nh tham sá»‘

```bash
# TÃ¹y chá»‰nh epochs, batch size
python main.py --models cnn_resnet_gru gru --epochs 500 --batch_size 32

# Thay Ä‘á»•i sá»‘ timesteps dá»± Ä‘oÃ¡n (output_steps)
python main.py --models cnn_resnet_gru --output_steps 10
# Choices: 5 (máº·c Ä‘á»‹nh), 10, 15, 20, 30, 40

# Train khÃ´ng cÃ³ noise
python main.py --models cnn_resnet_gru --no_noise

# Thay Ä‘á»•i output directory
python main.py --models cnn_resnet_gru --output_dir my_results

# Train vá»›i sensor khÃ¡c
python main.py --models cnn_resnet_gru --sensor_idx 1

# Káº¿t há»£p nhiá»u tham sá»‘
python main.py --models cnn_resnet_gru --output_steps 20 --epochs 1000 --batch_size 128
```

### 4. Ablation Study - So sÃ¡nh Model Variants

Train cÃ¡c model variants Ä‘á»ƒ phÃ¢n tÃ­ch contribution cá»§a tá»«ng component:

```bash
# Test 1: Loáº¡i bá» Residual Connection
python main.py --models cnn_gru --epochs 500 --output_steps 10

# Test 2: Loáº¡i bá» GRU layers
python main.py --models cnn_resnet --epochs 500 --output_steps 10

# Test 3: ThÃªm BatchNorm vÃ  Dropout
python main.py --models cnn_resnet_gru_bn --epochs 500 --output_steps 10

# Test 4: Sá»‘ lÆ°á»£ng GRU layers khÃ¡c nhau
python main.py --models cnn_resnet_gru_var --num_gru_layers 1 --epochs 500 --output_steps 10
python main.py --models cnn_resnet_gru_var --num_gru_layers 2 --epochs 500 --output_steps 10
python main.py --models cnn_resnet_gru_var --num_gru_layers 4 --epochs 500 --output_steps 10

# Train Táº¤T Cáº¢ ablation variants cÃ¹ng lÃºc
python main.py --models cnn_resnet_gru cnn_gru cnn_resnet cnn_resnet_gru_bn --epochs 500 --output_steps 10
```

**Má»¥c Ä‘Ã­ch Ablation Study:**
- âŒ **CNN+GRU** (no residual): ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a **residual connection** â†’ Î”RMSE = ?
- âŒ **CNN+ResNet** (no GRU): ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a **GRU layers** â†’ Î”RMSE = ?
- â• **CNN+ResNet+GRU+BN**: ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a **regularization** â†’ Î”RMSE = ?
- ğŸ”¢ **Variable Depth (1L, 2L, 4L)**: ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a **model depth** â†’ Best depth = ?

Sau khi train, phÃ¢n tÃ­ch káº¿t quáº£:
```bash
python analyze_existing_results.py --results_dir results --plot_predictions
```

### 5. Sá»­ dá»¥ng Cache (Tiáº¿t kiá»‡m thá»i gian)

**Cache Ä‘Æ°á»£c báº­t máº·c Ä‘á»‹nh** - Preprocessed data sáº½ Ä‘Æ°á»£c lÆ°u láº¡i vÃ  tÃ¡i sá»­ dá»¥ng!

```bash
# Láº§n Ä‘áº§u: Preprocess vÃ  lÆ°u cache (~30s)
python main.py --models cnn_resnet_gru

# Láº§n sau: Load tá»« cache (~1s) - NHANH HÆ N 30 Láº¦N!
python main.py --models gru

# Táº¯t cache (preprocess láº¡i tá»« Ä‘áº§u)
python main.py --models cnn_resnet_gru --no_cache

# XÃ³a táº¥t cáº£ cache trÆ°á»›c khi cháº¡y
python main.py --models cnn_resnet_gru --clear_cache
```

**LÆ°u Ã½:** Cache dá»±a trÃªn `sensor_idx`, `output_steps`, `add_noise`, `input_steps`. Thay Ä‘á»•i báº¥t ká»³ tham sá»‘ nÃ o sáº½ táº¡o cache má»›i.

### 6. Xem táº¥t cáº£ options

```bash
python main.py --help
```

## Cáº¥u hÃ¬nh

Chá»‰nh sá»­a `config.py` Ä‘á»ƒ thay Ä‘á»•i:
- ÄÆ°á»ng dáº«n data
- Hyperparameters (learning rate, batch size, epochs...)
- Kiáº¿n trÃºc model (sá»‘ layers, units...)
- Data split ratios

## Data Augmentation Strategies (NEW)

**Response to Reviewer Feedback**: Äá»ƒ test robustness cá»§a model trÃªn nhiá»u ká»‹ch báº£n nhiá»…u khÃ¡c nhau, project Ä‘Ã£ Ä‘Æ°á»£c má»Ÿ rá»™ng vá»›i multiple augmentation strategies.

### Available Strategies

#### 1. **Multiple Noise Levels** (Enhanced)
Test vá»›i nhiá»u má»©c Ä‘á»™ nhiá»…u khÃ¡c nhau thay vÃ¬ chá»‰ 1 má»©c (Ïƒ = 0.1 Ã— std):

```bash
# Test vá»›i nhiá»u noise levels: [0.05, 0.1, 0.15, 0.2]
python main.py --models cnn_resnet_gru \
    --use_multiple_noise_levels \
    --noise_factors 0.05 0.1 0.15 0.2 \
    --output_dir results/multi_noise_test
```

#### 2. **Random Dropout of Segments** (NEW)
Simulate missing data segments do transmission errors:

```bash
# Sá»­ dá»¥ng dropout augmentation
python main.py --models cnn_resnet_gru \
    --augmentation_strategies noise dropout \
    --dropout_prob 0.1 \
    --output_dir results/dropout_test
```

#### 3. **Block Missingness** (NEW)
Simulate sensor failures vá»›i large missing blocks:

```bash
# Sá»­ dá»¥ng block missingness
python main.py --models cnn_resnet_gru \
    --augmentation_strategies noise block_missingness \
    --block_miss_prob 0.05 \
    --block_miss_fill_method interpolate \
    --output_dir results/block_miss_test
```

#### 4. **Combined Strategies** (Comprehensive Test)
Test vá»›i táº¥t cáº£ strategies Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ robustness toÃ n diá»‡n:

```bash
# Comprehensive robustness test
python main.py --models cnn_resnet_gru \
    --augmentation_strategies noise dropout block_missingness \
    --use_multiple_noise_levels \
    --noise_factors 0.05 0.1 0.15 0.2 \
    --output_dir results/robustness_test
```

### Test Augmentation Strategies

Cháº¡y demo script Ä‘á»ƒ visualize cÃ¡c augmentation strategies:

```bash
python test_augmentations.py
```

**Output**: 5 PNG files minh há»a tá»«ng strategy

### Configuration Parameters

Trong `config.py`:

```python
# Multiple noise levels
USE_MULTIPLE_NOISE_LEVELS = False  # Báº­t Ä‘á»ƒ test nhiá»u má»©c Ä‘á»™
NOISE_FACTORS = [0.05, 0.1, 0.15, 0.2]

# Augmentation strategies
AUGMENTATION_STRATEGIES = ['noise']  # Options: 'noise', 'dropout', 'block_missingness'

# Random dropout
DROPOUT_PROB = 0.1
DROPOUT_MIN_LENGTH = 1
DROPOUT_MAX_LENGTH = 5

# Block missingness
BLOCK_MISS_PROB = 0.05
BLOCK_MISS_MIN_LENGTH = 3
BLOCK_MISS_MAX_LENGTH = 10
BLOCK_MISS_FILL_METHOD = 'interpolate'  # Options: 'zero', 'mean', 'interpolate'
```

**ğŸ“– Detailed Guide**: Xem `AUGMENTATION_GUIDE.md` cho chi tiáº¿t vÃ  best practices

## Káº¿t quáº£

Model sáº½ lÆ°u vÃ o `results/` (hoáº·c folder báº¡n chá»‰ Ä‘á»‹nh):
- `model_saved.keras`: Model weights
- `history_saved.pkl`: Training history
- `scaler_values.npy`: Scaler parameters
- `metrics.csv`: Evaluation metrics
- `loss_plot.png`, `mae_plot.png`: Training plots

## Performance

### Main Models Performance (output_steps=5)

| Model | RÂ² (Test) | RMSE | MAE |
|-------|-----------|------|-----|
| **CNN+ResNet+GRU** | **0.976** | 0.0010 | 0.0007 |
| GRU | 0.963 | 0.0013 | 0.0008 |
| XGBoost | 0.904 | 0.0019 | 0.0012 |
| LightGBM | 0.894 | 0.0021 | 0.0013 |
| CNN | 0.867 | 0.0023 | 0.0016 |
| Linear Regression | 0.867 | 0.0024 | 0.0017 |

### Ablation Study Results

Sau khi train cÃ¡c ablation models, báº¡n cÃ³ thá»ƒ so sÃ¡nh Ä‘á»ƒ xem:
- **Impact cá»§a Residual Connection**: So sÃ¡nh CNN+ResNet+GRU vs CNN+GRU
- **Impact cá»§a GRU Layers**: So sÃ¡nh CNN+ResNet+GRU vs CNN+ResNet
- **Impact cá»§a BatchNorm/Dropout**: So sÃ¡nh CNN+ResNet+GRU vs CNN+ResNet+GRU+BN
- **Optimal Model Depth**: So sÃ¡nh cÃ¡c variants 1L, 2L, 3L, 4L

PhÃ¢n tÃ­ch báº±ng:
```bash
python analyze_existing_results.py --results_dir results
```

## Äáº·c Ä‘iá»ƒm ká»¹ thuáº­t

### Residual Network (Skip Connection)
CNN+ResNet+GRU model sá»­ dá»¥ng skip connection giá»¯a input vÃ  Conv1D output:
```python
conv_out = Conv1D(64, kernel_size=3, activation='relu')(input_layer)
input_resized = Conv1D(64, kernel_size=1, activation='linear')(input_layer)
conv_out = Add()([conv_out, input_resized])  # Residual connection
```

### Model Architecture Comparison

| Component | CNN+ResNet+GRU | CNN+GRU | CNN+ResNet | CNN | GRU |
|-----------|----------------|---------|------------|-----|-----|
| **Conv1D Layer** | âœ… | âœ… | âœ… | âœ… | âŒ |
| **Residual Connection** | âœ… | âŒ | âœ… | âŒ | âŒ |
| **GRU Layers (3)** | âœ… | âœ… | âŒ | âŒ | âœ… |
| **BatchNorm/Dropout** | âŒ* | âŒ | âŒ | âœ… | âŒ |

*Sá»­ dá»¥ng `cnn_resnet_gru_bn` Ä‘á»ƒ thÃªm BatchNorm/Dropout vÃ o full model.

### Data Leakage Prevention

Dá»± Ã¡n nÃ y **ÄÃƒ Sá»¬A** cÃ¡c váº¥n Ä‘á» data leakage phá»• biáº¿n:

**âœ… Flow ÄÃšNG (TrÃ¡nh Data Leakage):**
```
1. Split data theo thá»i gian (train/val/test) - TRÆ¯á»šC
2. Augmentation (add noise) CHá»ˆ trÃªn TRAIN data - SAU
3. Val/Test data GIá»® NGUYÃŠN (khÃ´ng augment)
4. Scaler fit CHá»ˆ trÃªn train sequences
5. Táº¡o sequences SAU khi split
```

**âŒ Flow SAI (CÃ³ Data Leakage):**
```
1. Augmentation trÃªn TOÃ€N Bá»˜ data
2. Split data (train/val/test)
â†’ Káº¿t quáº£: CÃ¹ng 1 máº«u xuáº¥t hiá»‡n á»Ÿ cáº£ train vÃ  test (báº£n gá»‘c + báº£n noisy)
```

**Chi tiáº¿t:**
- **Temporal Split**: Data split theo thá»i gian (60/20/20), **KHÃ”NG shuffle**
- **Augmentation**: CHá»ˆ Ã¡p dá»¥ng cho train data (val/test giá»¯ nguyÃªn)
- **Scaler Fitting**: Fit CHá»ˆ trÃªn train sequences, apply lÃªn val/test
- **Sequence Creation**: Táº¡o sau khi split data (khÃ´ng táº¡o trÆ°á»›c)

## So sÃ¡nh Models vÃ  Output Steps

### Quick Analysis (30 giÃ¢y)

```bash
# PhÃ¢n tÃ­ch vÃ  so sÃ¡nh táº¥t cáº£ models vá»›i cÃ¡c output_steps
python analyze_existing_results.py
```

Táº¡o ra:
- `comparison_table.csv` - Báº£ng so sÃ¡nh Ä‘áº§y Ä‘á»§
- `metrics_vs_output_steps.png` - Line charts (RÂ², RMSE, MAE)
- `heatmaps.png` - Heatmaps cho visual comparison
- `best_configurations.csv` - Best configs cho tá»«ng metric
- `summary_report.txt` - BÃ¡o cÃ¡o chi tiáº¿t

### Full Analysis vá»›i Prediction Visualizations (3-5 phÃºt)

```bash
# PhÃ¢n tÃ­ch metrics + váº½ prediction comparisons
python analyze_existing_results.py --plot_predictions

# TÃ¹y chá»‰nh sá»‘ samples
python analyze_existing_results.py --plot_predictions --num_samples 3
```

Táº¡o thÃªm:
- â­ `predictions_comparison/overlay_out*.png` - **Overlay 3 models (KHUYÃŠN XEM)**
- `predictions_comparison/comparison_out*.png` - So sÃ¡nh models (3 subplots)
- `predictions_comparison/comparison_*.png` - So sÃ¡nh output_steps theo model
- `predictions_comparison/grid_sample*.png` - Grid tá»•ng quan

**Overlay plots:** Format giá»‘ng `prediction_sample_1.png` vá»›i Past Data + Actual + Cáº¢ 3 predictions overlay!

**Xem chi tiáº¿t:**
- `QUICK_COMPARISON.md` - HÆ°á»›ng dáº«n nhanh
- `PREDICTION_COMPARISON_GUIDE.md` - HÆ°á»›ng dáº«n chi tiáº¿t predictions
- `COMPARISON_GUIDE.md` - HÆ°á»›ng dáº«n train tá»« Ä‘áº§u

## TÃªn Model Thá»‘ng Nháº¥t

Trong cÃ¡c biá»ƒu Ä‘á»“ vÃ  bÃ¡o cÃ¡o, tÃªn model Ä‘Æ°á»£c chuáº©n hÃ³a nhÆ° sau:

| Model Type (Code) | TÃªn Hiá»ƒn Thá»‹ (Charts/Reports) |
|-------------------|-------------------------------|
| `cnn_resnet_gru` | **CNN+ResNet+GRU** |
| `cnn_gru` | **CNN+GRU** |
| `cnn_resnet` | **CNN+ResNet** |
| `cnn_resnet_gru_bn` | **CNN+ResNet+GRU+BN** |
| `cnn_resnet_gru_var` | **CNN+ResNet+GRU (XL)** (X = sá»‘ layers) |
| `cnn` | **CNN** |
| `gru` | **GRU** |
| `linear` | **Linear Regression** |
| `xgboost` | **XGBoost** |
| `lightgbm` | **LightGBM** |

TÃªn nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng nháº¥t quÃ¡n trong:
- âœ… Line charts (metrics vs output_steps)
- âœ… Heatmaps
- âœ… Comparison tables
- âœ… Summary reports
- âœ… Prediction plots

## TÃ¡c giáº£

Project nghiÃªn cá»©u vá» Time Series Forecasting cho dá»¯ liá»‡u rung Ä‘á»™ng cÃ´ng nghiá»‡p.
